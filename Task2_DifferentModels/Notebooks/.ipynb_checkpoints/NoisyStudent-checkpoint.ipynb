{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Student Model\n",
    "Paper: Self-training with Noisy Student improves ImageNet classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Import libraries     #################################\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from medmnist.models import ResNet18, ResNet50\n",
    "from medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, \\\n",
    "    BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n",
    "from medmnist.evaluator import getAUC, getACC, save_results\n",
    "from medmnist.info import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: CUDA not found, CPU only.\n"
     ]
    }
   ],
   "source": [
    "####################        Test if an GPU is available     #################################\n",
    "if torch.cuda.is_available():     \n",
    "    print('used GPU: ' + torch.cuda.get_device_name(0))\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "    kwar = {'num_workers': 8, 'pin_memory': True}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: CUDA not found, CPU only.\")\n",
    "    dev = torch.device(\"cpu\")\n",
    "    kwar = {}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "\n",
    "np.random.seed(551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Main function        #################################\n",
    "def main(flag, input_root, output_root, end_epoch, trainSize, download):\n",
    "    ''' main function\n",
    "    :param flag: name of subset\n",
    "\n",
    "    '''\n",
    "\n",
    "    flag_to_class = {\n",
    "        \"pathmnist\": PathMNIST,\n",
    "        \"chestmnist\": ChestMNIST,\n",
    "        \"dermamnist\": DermaMNIST,\n",
    "        \"octmnist\": OCTMNIST,\n",
    "        \"pneumoniamnist\": PneumoniaMNIST,\n",
    "        \"retinamnist\": RetinaMNIST,\n",
    "        \"breastmnist\": BreastMNIST,\n",
    "        \"organmnist_axial\": OrganMNISTAxial,\n",
    "        \"organmnist_coronal\": OrganMNISTCoronal,\n",
    "        \"organmnist_sagittal\": OrganMNISTSagittal,\n",
    "    }\n",
    "    DataClass = flag_to_class[flag]\n",
    "\n",
    "    info = INFO[flag]\n",
    "    task = info['task']\n",
    "    n_channels = info['n_channels']\n",
    "    n_classes = len(info['label'])\n",
    "\n",
    "    start_epoch = 0\n",
    "    lr = 0.001\n",
    "    batch_size = 8\n",
    "    val_auc_list = []\n",
    "    train_dataset_scaled = []\n",
    "    train_dataset_unlabeled = []\n",
    "\n",
    "    if trainSize != 1:\n",
    "        flag = flag + \"_\" + str(trainSize)\n",
    "\n",
    "    dir_path = os.path.join(output_root, '%s_checkpoints' % (flag))\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    print('==> Preparing data...')\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize(224),transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[.5], std=[.5])])\n",
    "\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize(224),transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[.5], std=[.5])])\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.Resize(224),transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[.5], std=[.5])])\n",
    "\n",
    "    train_dataset = DataClass(root=input_root,\n",
    "                                    split='train',\n",
    "                                    transform=train_transform,\n",
    "                                    download=download)\n",
    "    \n",
    "    random_data = torch.randperm(len(train_dataset))\n",
    "    indices = random_data[:round(len(train_dataset)*trainSize)]\n",
    "    indices_unlabeled = random_data[round(len(train_dataset)*trainSize):]\n",
    "    \n",
    "    for idx in indices:\n",
    "        train_dataset_scaled.append(train_dataset[idx])\n",
    "    for idx_un in indices_unlabeled:\n",
    "        train_dataset_unlabeled.append(train_dataset[idx_un])\n",
    "\n",
    "    \n",
    "    train_loader = data.DataLoader(dataset=train_dataset_scaled,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "    val_dataset = DataClass(root=input_root,\n",
    "                                  split='val',\n",
    "                                  transform=val_transform,\n",
    "                                  download=download)\n",
    "    val_loader = data.DataLoader(dataset=val_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "    test_dataset = DataClass(root=input_root,\n",
    "                                   split='test',\n",
    "                                   transform=test_transform,\n",
    "                                   download=download)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "    print(train_dataset_scaled[0][0])\n",
    "    print('Train: ', len(train_dataset_scaled), ', Valid: ', len(val_dataset), ', Test: ', len(test_dataset))\n",
    "    print('==> Building and training model...')\n",
    "\n",
    "    if torch.cuda.is_available():     \n",
    "        print('used GPU: ' + torch.cuda.get_device_name(0))\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        kwar = {'num_workers': 8, 'pin_memory': True}\n",
    "        cpu = torch.device(\"cpu\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: CUDA not found, CPU only.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        kwar = {}\n",
    "        cpu = torch.device(\"cpu\")\n",
    "    \n",
    "    #model = ResNet18(in_channels=n_channels, num_classes=n_classes).to(device)\n",
    "    model = EfficientNet.from_name('efficientnet-b7')\n",
    "    # Unfreeze model weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Linear(num_ftrs, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    if task == \"multi-label, binary-class\":\n",
    "        #criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion = nn.BCELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in trange(start_epoch, end_epoch):\n",
    "        train(model, optimizer, criterion, train_loader, device, task)\n",
    "        val(model, val_loader, device, val_auc_list, task, dir_path, epoch)\n",
    "\n",
    "    auc_list = np.array(val_auc_list)\n",
    "    index = auc_list.argmax()\n",
    "    print('epoch %s is the best model' % (index))\n",
    "\n",
    "    print('==> Testing model...')\n",
    "    restore_model_path = os.path.join(\n",
    "        dir_path, 'ckpt_%d_auc_%.5f.pth' % (index, auc_list[index]))\n",
    "    model.load_state_dict(torch.load(restore_model_path)['net'])\n",
    "    test(model,\n",
    "         'train',\n",
    "         train_loader,\n",
    "         device,\n",
    "         flag,\n",
    "         task,\n",
    "         output_root=output_root)\n",
    "    test(model, 'val', val_loader, device, flag, task, output_root=output_root)\n",
    "    test(model,\n",
    "         'test',\n",
    "         test_loader,\n",
    "         device,\n",
    "         flag,\n",
    "         task,\n",
    "         output_root=output_root)\n",
    "    \n",
    "    shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Training function        #################################\n",
    "def train(model, optimizer, criterion, train_loader, device, task):\n",
    "    ''' training function\n",
    "    :param model: the model to train\n",
    "    :param optimizer: optimizer used in training\n",
    "    :param criterion: loss function\n",
    "    :param train_loader: DataLoader of training set\n",
    "    :param device: cpu or cuda\n",
    "    :param task: task of current dataset, binary-class/multi-class/multi-label, binary-class\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32).to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long().to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Validation function        #################################\n",
    "def val(model, val_loader, device, val_auc_list, task, dir_path, epoch):\n",
    "    ''' validation function\n",
    "    :param model: the model to validate\n",
    "    :param val_loader: DataLoader of validation set\n",
    "    :param device: cpu or cuda\n",
    "    :param val_auc_list: the list to save AUC score of each epoch\n",
    "    :param task: task of current dataset, binary-class/multi-class/multi-label, binary-class\n",
    "    :param dir_path: where to save model\n",
    "    :param epoch: current epoch\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32).to(device)\n",
    "                m = nn.Sigmoid()\n",
    "                outputs = m(outputs).to(device)\n",
    "            else:\n",
    "                targets = targets.squeeze().long().to(device)\n",
    "                m = nn.Softmax(dim=1)\n",
    "                outputs = m(outputs).to(device)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "                \n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "            print(\"y_true: \", y_true)\n",
    "            print(\"y_score\", y_score)\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_score = y_score.detach().cpu().numpy()\n",
    "        auc = getAUC(y_true, y_score, task)\n",
    "        val_auc_list.append(auc)\n",
    "\n",
    "    state = {\n",
    "        'net': model.state_dict(),\n",
    "        'auc': auc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "\n",
    "    path = os.path.join(dir_path, 'ckpt_%d_auc_%.5f.pth' % (epoch, auc))\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Testing function        #################################\n",
    "def test(model, split, data_loader, device, flag, task, output_root=None):\n",
    "    ''' testing function\n",
    "    :param model: the model to test\n",
    "    :param split: the data to test, 'train/val/test'\n",
    "    :param data_loader: DataLoader of data\n",
    "    :param device: cpu or cuda\n",
    "    :param flag: subset name\n",
    "    :param task: task of current dataset, binary-class/multi-class/multi-label, binary-class\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32).to(device)\n",
    "                m = nn.Sigmoid()\n",
    "                outputs = m(outputs).to(device)\n",
    "            else:\n",
    "                targets = targets.squeeze().long().to(device)\n",
    "                m = nn.Softmax(dim=1)\n",
    "                outputs = m(outputs).to(device)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_score = y_score.detach().cpu().numpy()\n",
    "        auc = getAUC(y_true, y_score, task)\n",
    "        acc = getACC(y_true, y_score, task)\n",
    "        print('%s AUC: %.5f ACC: %.5f' % (split, auc, acc))\n",
    "\n",
    "        if output_root is not None:\n",
    "            output_dir = os.path.join(output_root, flag)\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.mkdir(output_dir)\n",
    "            output_path = os.path.join(output_dir, '%s.csv' % (split))\n",
    "            save_results(y_true, y_score, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################        Pseudo-Labeling function        #################################\n",
    "def pseudolabel(model, split, data_loader, device, flag, task, output_root=None):\n",
    "    ''' pseudo labeling function\n",
    "    :param model: the model to pseudo label\n",
    "    :param split: the data which will be labeled, 'train/val/test'\n",
    "    :param data_loader: DataLoader of data\n",
    "    :param device: cpu or cuda\n",
    "    :param flag: subset name\n",
    "    :param task: task of current dataset, binary-class/multi-class/multi-label, binary-class\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                m = nn.Sigmoid()\n",
    "                outputs = m(outputs).to(device)\n",
    "            else:\n",
    "                m = nn.Softmax(dim=1)\n",
    "                outputs = m(outputs).to(device)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_score = y_score.detach().cpu().numpy()\n",
    "        auc = getAUC(y_true, y_score, task)\n",
    "        acc = getACC(y_true, y_score, task)\n",
    "        print('%s AUC: %.5f ACC: %.5f' % (split, auc, acc))\n",
    "\n",
    "        if output_root is not None:\n",
    "            output_dir = os.path.join(output_root, flag)\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.mkdir(output_dir)\n",
    "            output_path = os.path.join(output_dir, '%s.csv' % (split))\n",
    "            save_results(y_true, y_score, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2e922abbdb0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retinamnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../../DataSets/medmnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../../TrainedNetsResNet18-NoisyStudent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-883a3d43af48>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(flag, input_root, output_root, end_epoch, trainSize, download)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mtest_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2882\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2885\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "main(\"retinamnist\", \"../../../DataSets/medmnist\", \"../../../TrainedNetsResNet18-NoisyStudent\", 2, 0.1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
